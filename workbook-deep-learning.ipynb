{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting SAP stock prices\n",
    "## Using PyTorch and SageMaker\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "> **Note**: Code and Markdown cells can be executed using the **Shift+Enter** keyboard shortcut. In addition, a cell can be edited by typically clicking it (double-click for Markdown cells) or by pressing **Enter** while it is highlighted.\n",
    "\n",
    "## General Outline\n",
    "\n",
    "Recall the general outline for SageMaker projects using a notebook instance.\n",
    "\n",
    "1. Part 1: Data preparation\n",
    "  1. Downloading data\n",
    "  2. Prepare the data\n",
    "  3. Divide into training and testing set\n",
    "  4. Scale/Normalize the data\n",
    "  5. Save the data\n",
    "2. Part 2: Local training and testing\n",
    "  1. Define a network model\n",
    "  1. Run training inside the notebook\n",
    "  2. Get insights regarding training quality\n",
    "3. Part 3: Use AWS for training and prediction\n",
    "  1. Prepare custom code for \n",
    "    1. Neural network model\n",
    "    2. Training\n",
    "  2. Upload the processed data to S3\n",
    "  3. Train the model on a separate GPU-optimized instance\n",
    "  4. Deploy the trained model on a separate instance\n",
    "  5. Use the model for predictions\n",
    "  6. Cleanup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer\n",
    "\n",
    "This is notebook is meant to demonstrate selected abilities of AWS in field of Machine Learning. \n",
    "\n",
    "Neural network used in this notebook is not optimized for prediction of stock prices. \n",
    "In fact, its predictions are very low quality. Predicting stock prices is a very difficult task! \n",
    "**Do not invest any money into stock based on these predictions**\n",
    "\n",
    "\n",
    "# Part 1: Data Preparation\n",
    "\n",
    "## Section 1.1: Downloading data\n",
    "\n",
    "We use the historical data from a polish provider Stooq.\n",
    "\n",
    "\n",
    "Visit https://stooq.com/q/d/?s=sap.de to fine-tune the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "\n",
    "# Global settings\n",
    "forecast_days = 1       # How many days to the future shall the stock price be predicted\n",
    "                        # For example: forecasting 2 days to the future will use the data from last days\n",
    "                        # to predict the price on a day 2 days ahead, but will not predict the price 1 day ahead\n",
    "\n",
    "batch_size = 10\n",
    "gradient_clip_maximum = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected commands similar to regular Linux commands can be used in the notebook.\n",
    "These commands are called *line magics*. List of available magics looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% lsmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Linux shell command can be run as well by preceding them with exclamation mark: `!`\n",
    "\n",
    "We use these tools to download the data from the source and store it in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p \"data\"\n",
    "# Clean existing data if necessary\n",
    "!rm -rf \"data/*\"\n",
    "\n",
    "!curl -k -sS \"https://stooq.com/q/d/l/?s=sap.de&i=d\" > \"data/data.raw.csv\"\n",
    "# i=d - interval daily (other: w,m,q,y - week, month, quarter, year)\n",
    "# o=1100000 - options - binary mask \n",
    "# other..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may want to start learning with simpler data, so here is the option to replace stock prices with `sin` signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_for_supervised_learning(src, target, shift=1):\n",
    "    \"\"\"\n",
    "    This function shifts target forward by `shift` positions with respect to the src\n",
    "    Example for shift=1:\n",
    "    [a,b,c] [a,b,c] -> [a,b,c] [x,a,b]\n",
    "    \"\"\"\n",
    "    src = pd.DataFrame(src)\n",
    "    target = pd.DataFrame(target)\n",
    "    \n",
    "    src = src.iloc[:(shift * -1)]\n",
    "    target = target.iloc[shift:]\n",
    "    \n",
    "    # Data was shifted and 'empty spaces' in form of NaNs have emerged\n",
    "    # We may fill it with zeros:\n",
    "    # src.fillna(0, inplace=True)\n",
    "    # or interpolate\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html#pandas.DataFrame.interpolate\n",
    "    src.interpolate(method='nearest', inplace=True)\n",
    "    target.interpolate(method='nearest', inplace=True)\n",
    "    \n",
    "    src.reset_index(inplace=True, drop=True)\n",
    "    target.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Preparing and Processing the data\n",
    "\n",
    "Default data range used at stooq is: September 1994 until today.\n",
    "\n",
    "Let's look at the data and discard any pieces of it that are not interesting for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stock_data(data_dir='data/'):  \n",
    "    ymd_date_parser = lambda x: pd.datetime.strptime(x, '%Y-%m-%d')\n",
    "    return pd.read_csv(data_dir+\"data.raw.csv\", parse_dates=['Date'], date_parser=ymd_date_parser)\n",
    "   \n",
    "    \n",
    "def select_columns(raw):\n",
    "    input_data = pd.DataFrame({\n",
    "        \"AvgPrice\": raw['Low'] + (raw['High'] - raw['Low'])/2\n",
    "    })\n",
    "    target_data = input_data.copy()\n",
    "    return input_data, target_data\n",
    "\n",
    "# Utility function to plot data quickly\n",
    "def plot_data(input_data, target_data, legend_1=\"left\", legend_2=\"right\", title=\"no title\", first_days=None):\n",
    "    plt.figure(figsize=(16,5))\n",
    "    x_axis = range(0,len(input_data))\n",
    "    plt.title(title)\n",
    "    plt.plot(x_axis[:first_days], input_data[:first_days], 'b.-', label=legend_1)\n",
    "    \n",
    "    if target_data is not None:\n",
    "        plt.plot(x_axis[:first_days], target_data[:first_days], 'r.-', label=legend_2)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = read_stock_data()\n",
    "print(\"Raw data imported from the source:\\n \", raw_data.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require columns of data containing the price of the stock. \n",
    "We calculate an average stock price based on the maximum and minimum from a given day.\n",
    "Rest of the data is discarded. \n",
    "\n",
    "First column should contain input, whereas the second column contains the target.\n",
    "The target column should be shifted forward by `forecast_days` in respect to the input column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, target_data = select_columns(raw_data)\n",
    "input_data, target_data = create_target_for_supervised_learning(input_data, target_data, forecast_days)\n",
    "\n",
    "print(\"Shape of input:\\n\", input_data.shape)\n",
    "print(\"Shape of target:\\n\", target_data.shape)\n",
    "\n",
    "print(\"Head of input:\\n\", input_data.head(5))\n",
    "print(\"Head of target:\\n\", target_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the data provides a chart depicting that the shift has succeeded. \n",
    "For any given day, the red curve represents the price of the stock from the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(input_data['AvgPrice'], \n",
    "          target_data['AvgPrice'], \n",
    "          \"Input\", \n",
    "          \"Ideal Prediction\",\n",
    "          \"Average Price unnormalized\", \n",
    "          first_days=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that both columns contain only numbers, thus we check if any `NaN`s or `None`s are present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data is clean\n",
    "# Check for NaN, None, and other non-numerical data\n",
    "print(\"There are {} null values in input\".format(input_data.isnull().values.sum()))\n",
    "print(\"There are {} null values in target\".format(target_data.isnull().values.sum()))\n",
    "\n",
    "# In case any NaNs or Nones are found, we query for their indexes\n",
    "columns = input_data.columns.values.tolist()\n",
    "for column in columns:\n",
    "    index = input_data[column].index[input_data[column].apply(np.isnan)]\n",
    "    print('Index {} in column {} is NaN or Null'.format(index,column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3: Divide Data into multiple sets\n",
    "\n",
    "Now we divide the data into two sets: training and testing.\n",
    "\n",
    "The training data set will be fed into network to train it.\n",
    "After training is complete, the test data set will be used to evaluate how well the network has been trained.\n",
    "\n",
    "### Datasets: Training, Validation, Test\n",
    "\n",
    "**Note**: Usually the data is divided onto three datasets: training, validation and testing.\n",
    "\n",
    "- `training` - used by the network in training,\n",
    "- `validation` - used by the network for evaluation (but not for training),\n",
    "- `test` - used by the user for evaluation\n",
    "\n",
    "The process runs usually like this:\n",
    "\n",
    "1. For each epoch do:\n",
    "  1. For each batch in the training dataset do:\n",
    "    1. Train the network using a batch\n",
    "    2. Use validation data to evaluate the quality of prediction of the network\n",
    "    3. Calculate loss (error) for training and validation\n",
    "  2. Evaluate stop criterion: either STOP or start NEXT EPOCH\n",
    "2. Finish training, store the model\n",
    "3. Deploy the model and use the test data to evaluate it\n",
    "\n",
    "Comparing the training loss against the validation loss allows to stop training at the right moment to prevent overfitting - i.e., learning by hard the right answers from the training data set.\n",
    "\n",
    "Simple stopping criterion could look like follows: *continue training as long as both training and validation loss is decreasing. Stop training when training loss keeps decreasing while validation loss stops decreasing.*\n",
    "\n",
    "### Our approach: Training + Test datasets\n",
    "\n",
    "Due to a small data set, we do not use the `validation` data set. For this reason, we may not use the stopping criterion as depicted above. Instead, we will stop training after a fixed number of epochs (cycles). This makes our network prone to overfitting.\n",
    "\n",
    "\n",
    "So lets, use first 95% of data as a training set and the last 5% of data as the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_stock_data(inputs, targets):\n",
    "    \"\"\"Prepare training and test sets from stock data.\"\"\"\n",
    "    \n",
    "    data_len = len(inputs)\n",
    "    # division point between train and test data\n",
    "    ratio = 0.95\n",
    "    division_index = int(data_len*ratio)\n",
    "    \n",
    "    inputs_train = inputs[:division_index]\n",
    "    inputs_test = inputs[division_index:]\n",
    "    \n",
    "    targets_train = targets[:division_index]\n",
    "    targets_test = targets[division_index:]\n",
    "    \n",
    "    return inputs_train, inputs_test, targets_train, targets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_orig, test_X_orig, train_y_orig, test_y_orig = divide_stock_data(input_data, target_data)\n",
    "print(\"Stock days in data set: train = {}, test = {}\".format(len(train_X_orig), len(test_X_orig)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.4: Scaling Data\n",
    "\n",
    "Data should be normalized to simplify the learning process.\n",
    "In this case, we use:\n",
    "- `StandardScaler` to standardize features by removing the mean and scaling to unit variance\n",
    "- `MinMaxScaler` to standardize features by placing all of them between 0 and 1\n",
    "\n",
    "\n",
    "### Note!\n",
    "\n",
    "We *fit* (configure) the scaler based on the training data, but we apply it to both: training and test.\n",
    "\n",
    "We do this to realistically simulate that we have never seen the test data before - we assume that all that we have available at this stage is the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of data\n",
    "\n",
    "# .copy() prevents warnings when replacing single columns in DataFrames\n",
    "# and allows to execute this cell multiple times without data overwritting\n",
    "train_X = train_X_orig.copy()\n",
    "train_y = train_y_orig.copy()\n",
    "test_X  = test_X_orig.copy() \n",
    "test_y  = test_y_orig.copy() \n",
    "\n",
    "# We fit scalers using training data and then apply scaling to both training and test datasets\n",
    "# We treat test data as we would never have seen it before\n",
    "\n",
    "# scaler_price = StandardScaler()\n",
    "scaler_price = MinMaxScaler()\n",
    "scaler_price.fit(train_X[['AvgPrice']].values)\n",
    "\n",
    "train_X[['AvgPrice']] = scaler_price.transform(train_X[['AvgPrice']])\n",
    "train_y[['AvgPrice']] = scaler_price.transform(train_y[['AvgPrice']])\n",
    "\n",
    "test_X[['AvgPrice']] = scaler_price.transform(test_X[['AvgPrice']])\n",
    "test_y[['AvgPrice']] = scaler_price.transform(test_y[['AvgPrice']])\n",
    "\n",
    "plot_data(train_X['AvgPrice'], train_y['AvgPrice'], \"Input\", \"Target\", \"Average Price training data normalized. 60 days\", 60)\n",
    "plot_data(train_X['AvgPrice'], train_y['AvgPrice'], \"Input\", \"Target\", \"Average Price training data normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note! \n",
    "The plot of the test data below contains values higher than `1.0`.\n",
    "This means that the test data set contains value that the network has never seen in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(test_X['AvgPrice'], test_y['AvgPrice'], \"Input\", \"Target\", \"Average Price test data normalized\")\n",
    "\n",
    "# How to go back to the unscaled price/volume\n",
    "denorm_X=scaler_price.inverse_transform(test_X[['AvgPrice']])\n",
    "denorm_y=scaler_price.inverse_transform(test_y[['AvgPrice']])\n",
    "\n",
    "plot_data(denorm_X, denorm_y, \"Input\", \"Target\", \"Average Price test data denormalized (y-axis in €)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start with training, let's double-check if the input and target data are correctly reflected in arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ideal prediction done at day 0:\")\n",
    "print(train_y[0:1])\n",
    "print(\"Should equal the average price from day 1:\")\n",
    "print(train_X[1:2])\n",
    "\n",
    "print(\"\\nThe same should hold for test data.\")\n",
    "print(\"Ideal prediction done at day 0:\")\n",
    "print(test_y[0:1])\n",
    "print(\"Should equal the average price from day 1:\")\n",
    "print(test_X[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.5: Save the data\n",
    "\n",
    "Training and test data are packed into a csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "\n",
    "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X)], axis=1) \\\n",
    "        .to_csv(os.path.join('data', 'train.csv'), header=False, index=False)\n",
    "\n",
    "\n",
    "pd.concat([pd.DataFrame(test_y), pd.DataFrame(test_X)], axis=1) \\\n",
    "        .to_csv(os.path.join('data', 'test.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Local training and testing\n",
    "\n",
    "\n",
    "## Section 2.1: Define a Neural Network Model\n",
    "\n",
    "### Brief introduction\n",
    "\n",
    "Neural network model can be seen as a Blackbox function that has inputs and outputs:\n",
    "\n",
    "![Blackbox Neural Network](images/black-box1.png)\n",
    "\n",
    "Each neural network has:\n",
    "- a structure of its blackbox\n",
    "- a set of parameters (weights) that configure the parts of its structure\n",
    "\n",
    "Process of training consists of setting the weights in such a way, so that prediction error is minimized.\n",
    "\n",
    "After the training is finished, the weights can be frozen and a trained model can be stored for use.\n",
    "\n",
    "Using of the model consists of providing the input and reading the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model used in our case: LSTM\n",
    "\n",
    "In our problem, we use a model consisting of LSTM (Long-short term memory) cells. **LSTM** is an example of recurrent neural network capable of learning long-term dependencies.\n",
    "\n",
    "Recurrent neural networks allow to take parts of their outputs as inputs in order to learn from the history and maintain a hidden state. The hidden state is a parametrized memory where the parameters describing the short-term memory and long-term memory can be learnt in the learning process.\n",
    "\n",
    "\n",
    "We define the inputs and outputs of the model as follows:\n",
    "\n",
    "- **Input** is a vector of past average stock prices. The length of this vector in configurable and will be called `batch_size`. It contains data from day `0` to day `batch_size - 1`. Today is `data[batch_size - 1]`\n",
    "- **Output** is also a vector of `batch_size` including average stock prices. It contains predictions for each elements of the input vector that are shifted by `forecast_days`, so that predictions for today are located in `predictions[batch_size - 2]` and for tomorrow in `predictions[batch_size - 1]` (under assumption that `forecast_days = 1`).\n",
    "\n",
    "![Blackbox Neural Network with Inputs and Outputs](images/black-box2.png)\n",
    "\n",
    "The Network looks conceptually as presented in Figure 3. Brief comments about the components:\n",
    "- In this diagram, time flows from left to right,\n",
    "- `A` block is a LSTM cell that takes input `x` and the state of the previous cell (right arrow between `A` blocks),\n",
    "- `h` is a hidden state, which is also an output of the LSTM layer,\n",
    "- LSTM layers can be stacked, so that output vector `h` can be the input to the next layer placed on top\n",
    "\n",
    "![RNN concept](images/RNN-unrolled.png)\n",
    "\n",
    "- *Image 3 source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/*\n",
    "\n",
    "### Note!\n",
    "\n",
    "We do not intend to go into details of LSTM. This is a relatively complex component.\n",
    "\n",
    "We want to show the training and deployment in the AWS environment provided that a definition of the network is given e.g., taken from from a paper.\n",
    "\n",
    "If you want to read more about LSTM, navigate to the following resources:\n",
    "- Understanding LSTM Networks:   [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) \n",
    "- LSTM processing animations:  [https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local training and testing\n",
    "\n",
    "Before we use our model on AWS, we would like to try it locally to make sure that the implementation is bug-free and initial results are correct. For this, we will train and evaluate the model in this notebook.\n",
    "\n",
    "First we will load the training data set to use. It may be time consuming to try and train the model completely in the notebook as we do not have access to a GPU and the compute instance that we are using is not particularly powerful. However, we can work on a portion the data to get a feel for how our training script is behaving.\n",
    "\n",
    "(In this case, we will use entire data set, as the network that we use requires little computational power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "The model is defined in `train/model.py` and the code looks as follows.\n",
    "\n",
    "We se there the following elements:\n",
    "- LSTM component, which can have multiple layers (parameter `self.num_layers`)\n",
    "- Linear component that takes the output from LSTM (of size `self.hidden_dim`) and coverts it to the size `output_dim` which in our case is `1`\n",
    "- function `init_hidden` that is responsible for initializing of the hidden layer in LSTM with zeros\n",
    "- function `forward` that defines how to produce the `output` from the `input` to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize train/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the training data and define size of the batch\n",
    "\n",
    "In this block, we read the previously stored training data from a csv file.\n",
    "\n",
    "We make sure that the number of samples from the training data set is a multiple of `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all data from file to calculate data length\n",
    "dummy_read = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None)\n",
    "num_rows_train = (len(dummy_read) // batch_size) * batch_size\n",
    "\n",
    "# Read again so many rows, so that num_rows/batch_size is an integer\n",
    "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=num_rows_train)\n",
    "\n",
    "# Order of data in train.csv:\n",
    "# Y-Average  |  X-Average  \n",
    "# to_predict |  INPUT VALUE\n",
    "\n",
    "# Turn the input in form of pandas.DataFrame into PyTorch Tensors\n",
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).float()\n",
    "\n",
    "#print(train_sample)\n",
    "print(train_sample_y.shape)\n",
    "print(train_sample_X.shape)\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the training method and testing it locally\n",
    "\n",
    "Next we need to write the training code itself. We train the network in **epochs** and **batches**.\n",
    "\n",
    "In a single *epoch*, the entire train dataset is fed to the network. We use multiple epochs for learning, so that the network can learn it better.\n",
    "\n",
    "In a single *batch* a subset of the training data is presented to the network. \n",
    "\n",
    "Longer the batch means usually:\n",
    "- better prediction results \n",
    "- less usability (more input is required to use the network)\n",
    "\n",
    "The code below is commented to understand the process a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model from the train/model.py file\n",
    "from train.model import LSTM\n",
    "   \n",
    "def train(model, train_loader, epochs, optimizer, criterion, device):\n",
    "    \n",
    "    # For storing the prediction error over epochs\n",
    "    loss_trace = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        \n",
    "        # Set model to train mode - allow gradient calculation and thus learning\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = len(train_loader)\n",
    "        \n",
    "        # Iterate over batches\n",
    "        for batch in train_loader:\n",
    "            batch_X, batch_y = batch\n",
    "            # Move data to GPU if available to speed-up calculations\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Remove gradients from previous batches from the memory. Network has already learnt it\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Obtain prediction for an input - run the `forward` function from the model definition\n",
    "            output = model(batch_X)\n",
    "           \n",
    "            # Calculate prediction error a.k.a loss\n",
    "            loss = criterion(output.squeeze(), batch_y)\n",
    "\n",
    "            # Apply back-propagation step\n",
    "            # Calculate corrections to the network, so that next time the error is (hopefully) smaller\n",
    "            loss.backward()\n",
    "            # Prevent `gradient explosion` phenomenon by clipping the gradients to a magic number\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_maximum)\n",
    "            \n",
    "            # Correct the weights of a network based on the corrections calculated previously\n",
    "            # Here the learning takes place\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Collect prediction error for this batch\n",
    "            total_loss += loss.data.item()\n",
    "            \n",
    "        avg_loss_per_batch = total_loss/num_batches\n",
    "        print(\"Epoch: {}, MSELoss per batch: {}\".format(epoch, avg_loss_per_batch))\n",
    "        loss_trace.append(avg_loss_per_batch)\n",
    "    return model, loss_trace\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: {}\".format(device))\n",
    "model = LSTM(input_dim=1, hidden_dim=24, batch_size=batch_size, output_dim=1, num_layers=2).to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "model, loss_trace = train(model, train_sample_dl, 15, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_loss_trace = np.array(loss_trace).reshape(-1, 1)\n",
    "\n",
    "plot_data(np_loss_trace, None, \"Loss\", \"\",\n",
    "          \"Avergage loss per batch for training epochs\", len(loss_trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposing we have the training method above, we will test that it is working by writing a bit of code in the notebook that executes our training method on the small sample training set that we loaded earlier. The reason for doing this in the notebook is so that we have an opportunity to fix any errors that arise early when they are easier to diagnose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dummy = pd.read_csv(os.path.join(data_dir, 'test.csv'), header=None, names=None)\n",
    "\n",
    "num_rows_test = (len(test_dummy) // batch_size) * batch_size\n",
    "\n",
    "test_sample = pd.read_csv(os.path.join(data_dir, 'test.csv'), header=None, names=None, nrows=num_rows_test)\n",
    "test_y = torch.from_numpy(test_sample[[0]].values).float().squeeze()\n",
    "\n",
    "test_X = torch.from_numpy(test_sample.drop([0], axis=1).values).float()\n",
    "\n",
    "# Build the dataset\n",
    "test_ds = torch.utils.data.TensorDataset(test_X, test_y)\n",
    "# Build the dataloader\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    data = dict()\n",
    "    data['prediction'] = []\n",
    "    data['expected'] = []\n",
    "\n",
    "    total_loss = 0\n",
    "    total_rmse = 0\n",
    "    num_batches = len(test_loader)\n",
    "\n",
    "    for batch in test_loader:\n",
    "        batch_X, batch_y = batch\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "    \n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output.squeeze(), batch_y)\n",
    "        \n",
    "        # Calculate errors - note copying data from GPU to CPU!\n",
    "        flat_output = output.cpu().detach().numpy().reshape(-1,1)              \n",
    "        flat_y = batch_y.cpu().detach().numpy()\n",
    "        data['prediction'].append(flat_output)\n",
    "        data['expected'].append(flat_y)\n",
    "        total_rmse += mean_squared_error(flat_y, flat_output)\n",
    "        total_loss += loss.data.item()\n",
    "\n",
    "        \n",
    "    print(\"test:mseloss: {}\".format(total_loss/num_batches))\n",
    "    print(\"test:rmse: {}\".format(total_rmse/num_batches))\n",
    "    return data\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "result = test(model, test_dl, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying prediction results\n",
    "\n",
    "Note that Y-axis is still normalized and some values are $> 1$.\n",
    "This means that they are `out of scale`, and the predictor will have problems with precise prediction in such cases, because it have never seen such training data before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove batching from result\n",
    "predictions = []\n",
    "for batch_arr in result['prediction']:\n",
    "    for value in batch_arr:\n",
    "        predictions.append(value)\n",
    "\n",
    "expectations = []\n",
    "for batch_arr in result['expected']:\n",
    "    for value in batch_arr:\n",
    "        expectations.append(value)\n",
    "\n",
    "plot_data(np.array(expectations).reshape(-1, 1), \n",
    "          np.array(predictions).reshape(-1, 1), \n",
    "          \"Expected prediction result\",\n",
    "          \"Obtained prediction result\",\n",
    "          \"Test data: prediction vs expectation - normalized\", len(expectations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invert scaling transformation\n",
    "\n",
    "Let's look at the chart with Y-axis expressed in euro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp = scaler_price.inverse_transform(np.array(expectations).reshape(-1, 1))\n",
    "pred = scaler_price.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "\n",
    "plot_data(exp, pred, \n",
    "          \"Expected prediction result\",\n",
    "          \"Obtained prediction result\",\n",
    "          \"Test data: prediction (red) expectation (blue) - denormalized in €\", len(expectations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1: Prepare Custom Code for Training and Prediction\n",
    "\n",
    "### Model as defined by Amazon SageMaker\n",
    "\n",
    "In particular, a model comprises three objects\n",
    "\n",
    " - Model Artifacts (provided in `train/model.py`),\n",
    " - Training Code (provided in `train/train.py`),\n",
    " - Inference Code (also provided in `train/train.py`).\n",
    " \n",
    "each of which interact with one another. Here we will still be using containers provided by Amazon with the added benefit of being able to include our own custom code.\n",
    "\n",
    "You already have seen the model in `train/model.py`. Parts of the training code were also used for local training. \n",
    "Let's peek into the training and inference code defined in `train/train.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize train/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2: Uploading the training data to S3\n",
    "\n",
    "\n",
    "Next, we need to upload the training data to the SageMaker default S3 bucket so that we can provide access to it while training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/stock-predictor-dojo1'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload entire data directory containing `train.csv` and `test.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.3: Train the model on a separate GPU-enabled EC2 instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to construct a PyTorch model using SageMaker we must provide SageMaker with a training script. We may optionally include a directory which will be copied to the container and from which our training code will be run. When the training container is executed it will check the uploaded directory (if there is one) for a `requirements.txt` file and install any required Python libraries, after which the training script will be run.\n",
    "\n",
    "We may also configure the type of EC2 instance that will be used for training. Here, we use one of the `p2.*` instances, which are GPU-enabled. We want to minimize the time we use the `p2` instance, because they are relatively expensive - cheapest one costs about $1 per hour.\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "Once a model is trained, we usually need much less computing power to use the model. Thus, we will later deploy the trained model on a weaker, non-GPU instance `m4.xlarge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type_training = 'ml.p2.xlarge'\n",
    "instance_type_deployment = 'ml.m4.xlarge'\n",
    "# Must be a 'ml.*'instance, for example:\n",
    "# - ml.m4.xlarge - new EC2 instance with CPU\n",
    "# - ml.p2.xlarge - new EC2 instance with GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metrics\n",
    "\n",
    "We may define custom metrics that AWS will parse and present in CloudWatch.\n",
    "The output from training based on a regular expression and present the data as a chart in CloudWatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metrics\n",
    "metric_definitions = [{'Name': 'Training-Loss',\n",
    "                       'Regex': 'train:mseloss: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining an Estimator\n",
    "\n",
    "Next, we define an `estimator` object with custom entry point (reference to our custom code) and optionally custom metrics definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type=instance_type_training,\n",
    "                    metric_definitions = metric_definitions,\n",
    "                    hyperparameters={\n",
    "                        'epochs': 50,\n",
    "                        'hidden-dim': 100,\n",
    "                        'batch-size': batch_size,\n",
    "                        'num-layers': 4,\n",
    "                        'learning-rate': 0.0001 # Keep not higher than 0.0002\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Next, we instruct to fit the estimator. Fitting is a better term to describe model learning :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with given hyperparameters\n",
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `Billable seconds: XX` output -  this is the total cost of running the training instance. \n",
    "This allows to run powerful multi-GPU instances at the same time minimizing the AWS costs.\n",
    "\n",
    "By navigating to training jobs (https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs) one can see various details like: monitoring of resources usage, potential errors, etc.\n",
    "\n",
    "The trained model is stored in S3, so that it can be deployed and used in a predictor later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.4: Deploy the Trained Model on a Separate Instance\n",
    "\n",
    "In order to use the model over a REST endpoint, we need to deploy it first.\n",
    "\n",
    "Deployed predictor is an EC2 instance holding the trained model. \n",
    "It can be accessed via `endpoint`. All endpoints are listed under: https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the trained model\n",
    "predictor = estimator.deploy(initial_instance_count=1, \n",
    "                         instance_type=instance_type_deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.5: Use the Deployed Model for Predictions\n",
    "\n",
    "Now the model is ready for use! Let's predict the price for tomorrow then!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get prices of SAP stocks from the last batch_size days\n",
    "test_data = pd.read_csv(os.path.join(data_dir, 'test.csv'), header=None, names=None)\n",
    "test_input = np.array(test_data[1])\n",
    "test_expected_result = np.array(test_data[0])\n",
    "\n",
    "# Slice the last batch_size days out of the test data\n",
    "input_1 = np.array(test_input[-1*batch_size:])\n",
    "expected_result_1 = np.array(test_expected_result[-1*batch_size:])\n",
    "\n",
    "print(\"Input to the predictor - scaled average prices from the last {} days:\\n{}\".format(batch_size,input_1))\n",
    "\n",
    "# Predict the prices\n",
    "result = predictor.predict(input_1)\n",
    "\n",
    "# Change the shape of the result from (1,X,1) to (X,1)\n",
    "result = np.array(result).reshape(-1, 1)\n",
    "\n",
    "# Denormalize the data, i.e., convert back to euro:\n",
    "scaled_result = scaler_price.inverse_transform(result[-1:]).item()\n",
    "print(\"Based on the trend from the last {} days, the price tomorrow will be: {}\".format(batch_size, scaled_result))\n",
    "\n",
    "input_scaled = scaler_price.inverse_transform(input_1.reshape(-1, 1))\n",
    "result_scaled = scaler_price.inverse_transform(result)\n",
    "expected_result_scaled = scaler_price.inverse_transform(expected_result_1.reshape(-1, 1))\n",
    "\n",
    "plot_data(input_scaled, \n",
    "          result_scaled, \n",
    "          \"Input - real price\",\n",
    "          \"Prediction - price next day\",\n",
    "          \"Input and output of the network - in €\", \n",
    "          len(input_scaled))\n",
    "\n",
    "plot_data(expected_result_scaled, \n",
    "          result_scaled, \n",
    "          \"Ideal prediction - price next day\",\n",
    "          \"Obtained prediction - price next day\",\n",
    "          \"Prediction quality - in €\", \n",
    "          len(result_scaled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the deployed model on the test data set\n",
    "\n",
    "Now, let's provide all batches from the test data set as input and analyze the prediction quality.\n",
    "\n",
    "Note, that we provide the network with the data that has not been used in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, rows=batch_size):\n",
    "    predictions = np.array([])\n",
    "    \n",
    "    batched = data.view(-1, batch_size)\n",
    "    for batch in batched:\n",
    "        result = predictor.predict(batch.numpy())\n",
    "        predictions = np.append(predictions, np.array(result))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test using all batches from the testing data set\n",
    "predictions = predict(test_X, batch_size)\n",
    "predictions = [num for num in np.array(predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean_squared_error when all predcitions = 1.0: \", mean_squared_error(test_y, np.ones(300)))\n",
    "print(\"mean_squared_error when all predcitions = 0.0: \", mean_squared_error(test_y, np.zeros(300)))\n",
    "print(\"mean_squared_error obtained result: \", mean_squared_error(test_y, np.array(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scaled = scaler_price.inverse_transform(test_y.numpy().reshape(-1, 1))\n",
    "result_scaled = scaler_price.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "\n",
    "plot_data(target_scaled, \n",
    "          result_scaled, \n",
    "          \"Ideal price prediction\",\n",
    "          \"Obtained price prediction\",\n",
    "          \"Test data: grasp on prediction quality - in €)\", \n",
    "          len(target_scaled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the deployed model on the **training** data set\n",
    "\n",
    "We may check if the model really well learned the data that was used for training.\n",
    "We expect here a bit of overfitting (learning the training data), so the prediction should be much better here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check how the predictor performs on the original training set\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None)\n",
    "train_input = np.array(train_data[1])\n",
    "train_expectation = np.array(train_data[0])\n",
    "\n",
    "input_2 = np.array(train_input[-1*batch_size:])\n",
    "train_expectation = np.array(train_expectation[-1*batch_size:].reshape(-1,1))\n",
    "\n",
    "predictions_train = predictor.predict(input_2)\n",
    "predictions_train = [num for num in np.array(predictions_train)]\n",
    "predictions_train = np.array(predictions_train).reshape(-1,1)\n",
    "\n",
    "print(\"mean_squared_error: \", mean_squared_error(train_expectation, predictions_train))\n",
    "\n",
    "target_scaled_train = scaler_price.inverse_transform(train_expectation)\n",
    "result_scaled_train = scaler_price.inverse_transform(predictions_train)\n",
    "\n",
    "plot_data(target_scaled_train, \n",
    "          result_scaled_train, \n",
    "          \"Ideal price prediction\",\n",
    "          \"Obtained price prediction\",\n",
    "          \"Training data: grasp on prediction quality - in €)\", \n",
    "          len(target_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you forget to delete the endpoint, the EC2 instance will be running and generating costs\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
